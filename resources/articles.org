#+TITLE: related work for the project
#+Author: 


* Code 
**  hugging face diffusers
- https://github.com/huggingface/diffusers
- Diffusers is the go-to library for state-of-the-art pretrained
  diffusion models for generating images, audio, and even 3D
  structures of molecules.

** audio-diffusion-pytorch (ETH + max-planck) 
- https://github.com/archinetai/audio-diffusion-pytorch
-  A fully featured audio diffusion library, for PyTorch. Includes
  models for unconditional audio generation, text-conditional audio
  generation, diffusion autoencoding, upsampling, and vocoding. The
  provided models are waveform-based, however, the U-Net (built using
  a-unet), DiffusionModel, diffusion method, and diffusion samplers
  are both generic to any dimension and highly customizable to work on
  other formats. Note: no pre-trained models are provided here, this
  library is meant for research purposes.

** dance diffusion (harmonai)
- https://colab.research.google.com/github/Harmonai-org/sample-generator/blob/main/Dance_Diffusion.ipynb?pli=1#scrollTo=lU97ZiP7nSKS
- Unconditional random audio sample generation
- Audio sample regeneration/style transfer using a single audio file
  or recording
- Audio interpolation between two audio files

** TODO diffusion lm and controllable music gen 
- https://github.com/SwordElucidator/Diffusion-LM-on-Symbolic-Music-Generation 
- Based on [[#Diffusion-LM Improves Controllable Text Generation]]
- This is a Stanford cs230 project by Hao Sun and Liwen Ouyang [[#Diffusion-LM on Symbolic Music Generation with Controllability (stanford)]]

** TODO FIGARO: Controllable Music Generation using Learned and Expert Features
- multitrack symbolic multi-track midi generator with expert-defined control features
- https://colab.research.google.com/drive/1UAKFkbPQTfkYMq1GxXfGZOJXOXU_svo6#scrollTo=Zsszt4J46OIj

* Literature 
** FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control
- https://arxiv.org/pdf/2201.10936.pdf

** FIGARO: CONTROLLABLE MUSIC GENERATION   USING EXPERT AND LEARNED FEATURES
- https://openreview.net/pdf/4ee95daea73cb05d2ea8780258b25684ccd82a88.pdf (more recent)

- Recent symbolic music generative models have achieved significant improvements
in the quality of the generated samples. Nevertheless, it remains hard for users
to control the output in such a way that it matches their expectation. To address
this limitation, high-level, human-interpretable conditioning is essential. In this
work, we release FIGARO, a Transformer-based conditional model trained to
generate symbolic music based on a sequence of high-level control codes. To this
end, we propose description-to-sequence learning, which consists of automatically
extracting fine-grained, human-interpretable features (the description) and training
a sequence-to-sequence model to reconstruct the original sequence given only the
description as input. FIGARO achieves state-of-the-art performance in multi-track
symbolic music generation both in terms of style transfer and sample quality. We
show that performance can be further improved by combining human-interpretable
with learned features. Our extensive experimental evaluation shows that FIGARO is
able to generate samples that closely adhere to the content of the input descriptions,
even when they deviate significantly from the training distribution

** Diffusion-LM Improves Controllable Text Generation
- https://arxiv.org/pdf/2205.14217.pdf
- Controlling the behavior of language models (LMs) without
  re-training is a major open problem in natural language
  generation. While recent works have demon- strated successes on
  controlling simple sentence attributes (e.g., sentiment), there has
  been little progress on complex, fine-grained controls (e.g.,
  syntactic structure).  To address this challenge, we develop a new
  non-autoregressive language model based on continuous diffusions
  that we call Diffusion-LM. Building upon the recent successes of
  diffusion models in continuous domains, Diffusion-LM iteratively
  denoises a sequence of Gaussian vectors into word vectors, yielding
  a sequence of intermediate latent variables. The continuous,
  hierarchical nature of these inter- mediate variables enables a
  simple gradient-based
 algorithm to perform complex, controllable generation tasks. We
 demonstrate successful control of Diffusion-LM for six challenging
 fine-grained control tasks, significantly outperforming prior work.

** Symbolic music generation conditioned on continuous-valued emotions
- https://arxiv.org/pdf/2203.16165.pdf
- In this paper we present a new approach for the generation of multi-instrument symbolic
 music driven by musical emotion. The principal novelty of our approach centres on conditioning a state-
 of-the-art transformer based on continuous-valued valence and arousal labels. In addition, we provide a
 new large-scale dataset of symbolic music paired with emotion labels in terms of valence and arousal. We
 evaluate our approach in a quantitative manner in two ways, first by measuring its note prediction accuracy,
 and second via a regression task in the valence-arousal plane. Our results demonstrate that our proposed
 approaches outperform conditioning using control tokens which is representative of the current state of the
 art

**  Noise2Music: Text-conditioned Music Generation with Diffusion Models
-  https://arxiv.org/abs/2302.03917
- https://google-research.github.io/noise2music/

- We introduce Noise2Music, where a series of diffusion models is
  trained to generate high-quality 30-second music clips from text
  prompts. Two types of diffusion models, a generator model, which
  generates an intermediate representation conditioned on text, and a
  cascader model, which generates high-fidelity audio conditioned on
  the intermediate representation and possibly the text, are trained
  and utilized in succession to generate high-fidelity music. We
  explore two options for the intermediate representation, one using a
  spectrogram and the other using audio with lower fidelity. We find
  that the generated audio is not only able to faithfully reflect key
  elements of the text prompt such as genre, tempo, instruments, mood,
  and era, but goes beyond to ground fine-grained semantics of the
  prompt. Pretrained large language models play a key role in this
  story -- they are used to generate paired text for the audio of the
  training set and to extract embeddings of the text prompts ingested
  by the diffusion models.

** DANCE2MIDI: DANCE-DRIVEN MULTI-INSTRUMENTS MUSIC GENERATION

-  https://www.catalyzex.com/paper/arxiv:2301.09080
- Dance-driven music generation aims to generate musical pieces
  conditioned on dance videos. Previous works focus on monophonic or
  raw audio generation, while the multi- instruments scenario is
  under-explored. The challenges of the dance-driven multi-instruments
  music (MIDI) genera- tion are two-fold: 1) no publicly available
  multi-instruments MIDI and video paired dataset and 2) the weak
  correla- tion between music and video. To tackle these challenges,
  we build the first multi-instruments MIDI and dance paired dataset
  (D2MIDI). Based on our proposed dataset, we in- troduce a
  multi-instruments MIDI generation framework (Dance2MIDI) conditioned
  on dance video. Specifically, 1) to model the correlation between
  music and dance, we encode the dance motion using the GCN, and 2) to
  generate harmo- nious and coherent music, we employ Transformer to
  decode the MIDI sequence. We evaluate the generated music of our
  framework trained on D2MIDI dataset and demonstrate that our method
  outperforms existing methods. The data and code are available on
  https://github.com/Dance2MIDI/Dance2MIDI

** Mo√ªsai: Text-to-Music Generation with Long-Context Latent Diffusion
- https://arxiv.org/pdf/2301.11757.pdf
** review of music generation 
- https://www.catalyzex.com/paper/arxiv:2211.09124

**  SYMBOLIC MUSIC GENERATION WITH DIFFUSION MODELS     
- https://arxiv.org/pdf/2103.16091.pdf

** Diffusion-LM on Symbolic Music Generation with Controllability (stanford)
- http://cs230.stanford.edu/projects_fall_2022/reports/16.pdf

** Generating Lead Sheets with Affect: A Novel Conditional seq2seq Framework
The authors proposed a novel approach to generate lead sheets (melody with cord progression) from scratch which are conditioned on musical valence, phrasing and time signature. They took inspiration from the Neural Machine Translation problem and made use of the sequence to sequence framework to translate a sequence of high-level musical parameters (conditions) to a lead sheet. A user can define high-level features (valence, phrasing and time signature) as well as a "low-level" event density called event density which describes the number of events in a bar.
Source: https://arxiv.org/abs/2104.13056

** MusicLM: Generating Music From Text
- MusicLM is a generative model that produces high-quality music consistent over several minutes conditioned on text sequence with additional conditional input possible such as melody or image. It uses three independent pretrained models to generate latent representations for audio and text and to calculate coherence between audio and text embeddings. The generation of music is described as a hierarchical sequence-to-sequence modeling task, with a semantic modeling stage to process conditioning signals and an acoustic modeling stage to translate both conditioning tokens and semantic tokesn to audio. 
- Source: https://arxiv.org/abs/2301.11325
- TEXT AND MELODY CONDITIONING EXAMPLE: https://google-research.github.io/seanet/musiclm/examples/
** Music SketchNet: Controllable Music Generation via Factorized Representations of Pitch and Rhythm
Music SketchNet is a neural network framework for automatic music generation guided by partial musical ideas from a user. The model is designed to fill in missing part based on known past and future context and "user specification" in form of text input. It uses SketchVAE, a novel variational autoencoder that explicitly factorizes rhythm and pitch contour, to encode and decode the music between external music measures and learned factorized latent representatations. The authors further introduced SketchInpainter, which predicts musical ideas in the form of latent variables and SketchConnector which combines the output from SketchInpainter and the user's sketching. The output of SketchConnector is then decoded by SketchVAE to generate music.
Source: https://arxiv.org/abs/2008.01291

** SongDriver: Real-time Music Accompaniment Generation without Logical Latency nor Exposure Bias
Real-time music accompaniment generation has a wide range of applications in the music industry, such as music education and live performances. However, automatic real-time music accompaniment generation is still understudied and often faces a trade-off between logical latency and exposure bias. In this paper, we propose SongDriver, a real-time music accompaniment generation system without logical latency nor exposure bias. Specifically, SongDriver divides one accompaniment generation task into two phases: 1) The arrangement phase, where a Transformer model first arranges chords for input melodies in real-time, and caches the chords for the next phase instead of playing them out. 2) The prediction phase, where a CRF model generates playable multi-track accompaniments for the coming melodies based on previously cached chords. With this two-phase strategy, SongDriver directly generates the accompaniment for the upcoming melody, achieving zero logical latency. Furthermore, when predicting chords for a timestep, SongDriver refers to the cached chords from the first phase rather than its previous predictions, which avoids the exposure bias problem. Since the input length is often constrained under real-time conditions, another potential problem is the loss of long-term sequential information. To make up for this disadvantage, we extract four musical features from a long-term music piece before the current time step as global information. In the experiment, we train SongDriver on some open-source datasets and an original √†iMusic Dataset built from Chinese-style modern pop music sheets. The results show that SongDriver outperforms existing SOTA (state-of-the-art) models on both objective and subjective metrics, meanwhile significantly reducing the physical latency.
Source: https://dl.acm.org/doi/10.1145/3503161.3548368

** AI-Based Affective Music Generation Systems: A Review of Methods, and Challenges
Music is a powerful medium for altering the emotional state of the listener. In recent years, with significant advancement in computing capabilities, artificial intelligence-based (AI-based) approaches have become popular for creating affective music generation (AMG) systems that are empowered with the ability to generate affective music. Entertainment, healthcare, and sensor-integrated interactive system design are a few of the areas in which AI-based affective music generation (AI-AMG) systems may have a significant impact. Given the surge of interest in this topic, this article aims to provide a comprehensive review of AI-AMG systems. The main building blocks of an AI-AMG system are discussed, and existing systems are formally categorized based on the core algorithm used for music generation. In addition, this article discusses the main musical features employed to compose affective music, along with the respective AI-based approaches used for tailoring them. Lastly, the main challenges and open questions in this field, as well as their potential solutions, are presented to guide future research. We hope that this review will be useful for readers seeking to understand the state-of-the-art in AI-AMG systems, and gain an overview of the methods used for developing them, thereby helping them explore this field in the future.
Source:https://arxiv.org/pdf/2301.06890.pdf

** From Words to Music: A Study of Subword Tokenization Techniques in Symbolic Music Generation
- Subword tokenization has been widely successful in text-based natural language processing (NLP) tasks with Transformer-based models. As Transformer models become increasingly popular in symbolic music-related studies, it is imperative to investigate the efficacy of subword tokenization in the symbolic music domain. In this paper, we explore subword tokenization techniques, such as byte-pair encoding (BPE), in symbolic music generation and its impact on the overall structure of generated songs. Our experiments are based on three types of MIDI datasets: single track-melody only, multi-track with a single instrument, and multi-track and multi-instrument. We apply subword tokenization on post-musical tokenization schemes and find that it enables the generation of longer songs at the same time and improves the overall structure of the generated music in terms of objective metrics like structure indicator (SI), Pitch Class Entropy, etc. We also compare two subword tokenization methods, BPE and Unigram, and observe that both methods lead to consistent improvements. Our study suggests that subword tokenization is a promising technique for symbolic music generation and may have broader implications for music composition, particularly in cases involving complex data such as multi-track songs.
- Source: https://arxiv.org/abs/2304.08953

** ComMU: Dataset for Combinatorial Music Generation
- Commercial adoption of automatic music composition requires the capability of generating diverse and high-quality music suitable for the desired context (e.g., music for romantic movies, action games, restaurants, etc.). In this paper, we introduce combinatorial music generation, a new task to create varying background music based on given conditions. Combinatorial music generation creates short samples of music with rich musical metadata, and combines them to produce a complete music. In addition, we introduce ComMU, the first symbolic music dataset consisting of short music samples and their corresponding 12 musical metadata for combinatorial music generation. Notable properties of ComMU are that (1) dataset is manually constructed by professional composers with an objective guideline that induces regularity, and (2) it has 12 musical metadata that embraces composers' intentions. Our results show that we can generate diverse high-quality music only with metadata, and that our unique metadata such as track-role and extended chord quality improves the capacity of the automatic composition. We highly recommend watching our video before reading the paper (https://pozalabs.github.io/ComMU).
- Source: https://arxiv.org/abs/2211.09385

* Available datasets
** giant-piano midi dataset
- GiantMIDI-Piano: A large-scale MIDI Dataset for
  Classical Piano Music
- https://arxiv.org/pdf/2010.07061.pdf
** mono midi transposition dataset 
- simpler dataset https://sebasgverde.github.io/mono-midi-transposition-dataset/

** The Lakh MIDI Dataset
- https://colinraffel.com/projects/lmd/#license
- collection of 176.581 MIDI files

** Lakh MIDI Dataset Clean
- https://www.kaggle.com/datasets/imsparsh/lakh-midi-clean?resource=download
- subset of the Lakh MIDI Dataset
- contains only files which names indicate their artist and title

** FMA (MP3 format)
- https://github.com/mdeff/fma
- 106.574 tracks of 161 unbalanced genres in MP3 format

** The MAESTRO Dataset
- https://magenta.tensorflow.org/datasets/maestro
- 200 hours of paired audio and MIDI recordings from ten years of International Piano-e-Competition

** MusicCaps
- https://research.google/resources/datasets/musiccaps/
- contains 5.521 music examples
- all are labeled with an English aspect list and a free text caption

** Los Angeles MIDI Dataset
- https://github.com/asigalov61/Los-Angeles-MIDI-Dataset
- contains of around 600.000 MIDIs with extensive meta-data

** The Expanded Groove MIDI Dataset
- https://magenta.tensorflow.org/datasets/e-gmd
- dataset of human drum perfomances as MIDI files
- contains 444 hours of audio from 43 drum kits

** ADL Piano MIDI
- https://paperswithcode.com/dataset/adl-piano-midi
- dataset of 11.086 piano pieces from different genres
- based on the Lakh MIDI dataset

** Some further datasets (some are mentioned here, some not)
- https://github.com/asigalov61/Tegridy-MIDI-Dataset
- contains a multi-instrumental MIDI dataset with almost 2.000 songs
- contains a list with their links of 15 other datsets

* diverse
** overview of different music gen methods 
-  https://www.catalyzex.com/s/music%20generation
